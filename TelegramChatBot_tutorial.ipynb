{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5b2bf8",
   "metadata": {},
   "source": [
    "# HOW TO INTEGRATE LLM-POWERED CHATBOT WITH A TELEGRAM FOR FREE\n",
    "(source: https://www.italiens.ee/en/blog/how_to_integrate_llm_powered_chatbot_with_telegram_for_free/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d38c3",
   "metadata": {},
   "source": [
    "# Before starting \n",
    "What You‚Äôll Need\n",
    "- A Telegram account\n",
    "- A Hugging Face account (free)\n",
    "- Basic Python setup (Python installed, and ideally a virtual environment set up)\n",
    "\n",
    "## Step 1: Create Your Telegram Bot\n",
    "1. Open Telegram on your device.\n",
    "2. Search for BotFather and click on the ‚òëÔ∏è verified account.\n",
    "3. Type /newbot and follow the instructions:\n",
    "    - Give your bot a name\n",
    "    - Create a username that ends with bot\n",
    "4. BotFather will send you a bot token. Copy this token ‚Äî you‚Äôll need it soon.\n",
    "\n",
    "## Step 2: Get a Free LLM API Key from Hugging Face\n",
    "1. Go to huggingface.co and sign up (free).\n",
    "2. Click your profile icon ‚Üí Access Tokens.\n",
    "3. Click New Token, give it a name (e.g., \"LLMBotToken\"), and create it with write access.\n",
    "4. Copy the API token.\n",
    "5. Choose a free model from the Hugging Face Model Hub. Free models have open license (e.g. MIT, Apache 2.0). In this guide we will use DeepSeek-R1-Distill-Qwen-1.5B.\n",
    "    - Keep in mind, this is a reasoning model, so it's quite verbose. You can \"play\" with it directly on the Hugging Face website, or choose another one.\n",
    "6. Click Deploy ‚Üí Inference Providers. Here you can choose an Inference Provider to deploy selected model and access sample code snippets for Python, JavaScript, and cURL.\n",
    "\n",
    "## Step 3: Install Required Libraries\n",
    "1. Create a new folder for your project and open it in your terminal.\n",
    "2. You will need to install the following libraries to interact with the Telegram bot API and Hugging Face:\n",
    "    ```ruby\n",
    "    pip install python-telegram-bot huggingface_hub\n",
    "    ```\n",
    "\n",
    "## Step 4: Prepare the Python Telegram Bot Script\n",
    "1. Create a file named bot.py.\n",
    "2. Paste the following code inside the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4725b5b",
   "metadata": {},
   "source": [
    "## First import libraries\n",
    "1. telegram / telegram.ext: librer√≠a oficial python-telegram-bot.\n",
    "\n",
    "    - Update: objeto que representa un evento entrante (mensaje, comando, etc.).\n",
    "    - ApplicationBuilder: constructor de la app del bot (reemplaza al viejo Updater en versiones modernas).\n",
    "    - CommandHandler: maneja comandos como /start.\n",
    "    - MessageHandler: maneja mensajes de texto normales.\n",
    "    - ContextTypes: tipo para el context que acompa√±a al update (√∫til para anotar tipos y acceder a data).\n",
    "    - filters: condiciones para MessageHandler (p. ej. filters.TEXT).\n",
    "\n",
    "2. InferenceClient de huggingface_hub: cliente para llamar a modelos alojados (aqu√≠ lo usan para chat/completions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ddd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "from telegram import Update\n",
    "from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters\n",
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c51aa5",
   "metadata": {},
   "source": [
    "## Declarar TOKENS\n",
    "- TELEGRAM_TOKEN es el Token del bot de Telegram\n",
    "- HF_API_KEY es el Token de Huggingface como inference provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11eebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TELEGRAM_TOKEN = 'YOUR_TELEGRAM_BOT_TOKEN'\n",
    "HF_API_KEY = 'YOUR_HF_API_TOKEN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ad894",
   "metadata": {},
   "source": [
    "## Inicializaci√≥n del cliente y almacenar conversaciones\n",
    "\n",
    "La funci√≥n InferenceClient(...) crea el cliente para llamar al servicio de inferencia. \"provider='novita'\" es un parametro de proveedor asociado al LLM elegido. \n",
    "\n",
    "La variable user_conversations = {} en un dict en memoria que mapea user_id con la lista de mensajes (historial para contexto). Ojo! Esto se borra al reiniciar y puede crecer indefinidamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\n",
    "    provider=\"novita\",\n",
    "    api_key=HF_API_KEY,\n",
    ")\n",
    "\n",
    "user_conversations = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c496bbea",
   "metadata": {},
   "source": [
    "## Handler del comando start \n",
    "\n",
    "Cuando un usuario manda /start, Telegram env√≠a un Update. La funci√≥n async (as√≠ncrona) recibe ese Update y update.message.reply_text(...) env√≠a texto de vuelta al usuario (aqu√≠ el bot da una bienvenida)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    await update.message.reply_text('Hello! Send me a message and I will reply using AI!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e6ada",
   "metadata": {},
   "source": [
    "## Funci√≥n main y arranque del bot\n",
    "\n",
    "- ApplicationBuilder().token(...).build(): crea la aplicaci√≥n del bot con tu token.\n",
    "- Registra handlers:\n",
    "    - /start ‚Üí start\n",
    "    - Mensajes de texto que no sean comandos ‚Üí handle_message (la expresi√≥n filters.TEXT & (~filters.COMMAND) filtra solo texto normal).\n",
    "- run_polling() inicia el \"polling\" ‚Äî el bot consulta a los servidores de Telegram continuamente por nuevos updates (alternativa: webhook).\n",
    "- if __name__ == '__main__': main() permite ejecutar el script como programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Crea una instancia de la aplicaci√≥n del bot que se conecta a los servidores de Telegram, escucha los mensajes entrantes, \n",
    "    # llama autom√©ticamente a las funciones que registres como \"handlers\"\n",
    "    app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()\n",
    "\n",
    "    #Los handlers son reglas que dicen: \"Cuando ocurra X tipo de mensaje, ejecuta esta funci√≥n\"\n",
    "    #CommandHandler es una funci√≥n especual que detecta comandos de telegram tipo /help, /start, etc. \n",
    "    #Esta l√≠nea le dice al bot \"Cuando un usuario escriba el comando /start (cuyo nombre es \"start\") llama a la funci√≥n start\"\n",
    "    app.add_handler(CommandHandler(\"start\", start))\n",
    "\n",
    "    #El message handler maneja mensajes de texto normales \n",
    "    #Le dice al bot \"cuando recibas un mensaje de texto que no sea un comando, llama a la funci√≥n handle_message\"\n",
    "    #filters.TEXT acepta mensajes de tipo texto \n",
    "    #~filters.COMMAND excluye los comandos (mensajes que empiezan por /)\n",
    "    #El operador & combina ambos filtros\n",
    "    app.add_handler(MessageHandler(filters.TEXT & (~filters.COMMAND), handle_message))\n",
    "\n",
    "    print(\"Bot is running...\")\n",
    "    app.run_polling()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda84a7b",
   "metadata": {},
   "source": [
    "## Handler para mensajes de texto (n√∫cleo del bot)\n",
    "1. Extrae el user_id (identificador del chat) y el ltexto del usuario. Y env√≠a un mensaje inmediato \"Thinking\". \n",
    "2. Si no existe un historial para ese user_id, crea la lista. \n",
    "3. Agrega el mensaje del usuario al historial como {\"role\": \"user\", \"content\": ...} ‚Äî este formato es el t√≠pico para APIs de chat (sistema/user/assistant).\n",
    "4. Llama a la API de Hugging Face via InferenceClient pidiendo una completaci√≥n de chat. Par√°metros: \n",
    "    - model: nombre del modelo \n",
    "    - messages: historial de conversaci√≥n (contexto)\n",
    "    - max_tokens: l√≠mite de tokens en la respuesta\n",
    "5. Extrae el texto de respiesta del LLM (choices[0]...)\n",
    "6. A√±ade la respuesta al historial para mantener contexto en futuros intercambios. \n",
    "7. Env√≠a la respuesta al usuario v√≠a reply_text\n",
    "8. Por √∫ltimo captura cualquier excpeci√≥n de la llamada a la API o de la l√≥gica y la notifica al usuario. \n",
    "\n",
    "Explicaci√≥n: await update.message.reply_text(\"Thinking...\")\n",
    "- update: es un objeto de tipo de telegram.Update que representa un evento entrante que el bot recibe desde telegram. Ejemplo de lo que puede contener: \n",
    "    update = {\n",
    "        \"update_id\": 123456,\n",
    "        \"message\": {\n",
    "            \"message_id\": 42,\n",
    "            \"from\": {\"id\": 987654321, \"first_name\": \"Alice\"},\n",
    "            \"chat\": {\"id\": 987654321, \"type\": \"private\"},\n",
    "            \"date\": 1698163837,\n",
    "            \"text\": \"Hola bot!\"\n",
    "        }\n",
    "    }\n",
    "- update.message: es el mensaje concreto que envi√≥ el usuario\n",
    "- reply_text() es un m√©todo del objeto Message cuyo prop√≥sito es responder a ese mensaje (en el mismo chat) enviando texto de vuelta al usuario. Internamente: \n",
    "    - Construye una petici√≥n HTTP a la API de telegram\n",
    "    - Espera la respuesta (por eso es as√≠ncrono)\n",
    "    - Usa el TELEGRAM_TOKEN para autentificar\n",
    "    - Env√≠a el texto \"Thinking\"\n",
    "    - Se pausa la ejecuci√≥n de la funci√≥n mientras se env√≠a el mensaje y se recibe la confirmaci√≥n\n",
    "\n",
    "Qu√© devuelve? \n",
    "- reply_text() devuelve un objeto Message que representa el mensaje que el bot acaba de enviar.\n",
    "- Esto te permitir√≠a: Guardar el ID del mensaje. Editarlo despu√©s (por ejemplo, cambiar ‚ÄúThinking‚Ä¶‚Äù por la respuesta final).\n",
    "    ```ruby\n",
    "    sent_msg = await update.message.reply_text(\"Thinking...\")\n",
    "    # unos segundos despu√©s\n",
    "    await sent_msg.edit_text(\"Aqu√≠ est√° tu respuesta final ‚ú®\")\n",
    "    ```\n",
    "\n",
    "EN resumen: update.message.reply_text(\"Thinking...\") ‚Üí le dice al servidor de Telegram ‚Äúenv√≠a este texto como respuesta al mensaje recibido‚Äù.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    user_id = update.message.chat_id\n",
    "    user_message = update.message.text\n",
    "\n",
    "    # Despu√©s de mandar el mensaje, el control vuelve al bucle principal mientras Telegram responde. Cuando responde, esta funci√≥n se reanuda\n",
    "    await update.message.reply_text('Thinking... ü§î')\n",
    "\n",
    "    #guardar el contexto seg√∫n el user_id permite que haya varios clientes a la vez guardando el contexto/historial por separado\n",
    "    if user_id not in user_conversations:\n",
    "        user_conversations[user_id] = []\n",
    "\n",
    "    user_conversations[user_id].append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    try:\n",
    "        #esta funci√≥n no es as√≠ncrona\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            messages=user_conversations[user_id],\n",
    "            max_tokens=500,\n",
    "        )\n",
    "\n",
    "        response_text = completion.choices[0].message[\"content\"]\n",
    "        user_conversations[user_id].append({\"role\": \"assistant\", \"content\": response_text})\n",
    "\n",
    "        # Al terminar, responde al usuario\n",
    "        await update.message.reply_text(response_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        await update.message.reply_text('Something went wrong! üò•')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGChatBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
