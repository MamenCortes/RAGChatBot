{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e5b2bf8",
   "metadata": {},
   "source": [
    "# HOW TO INTEGRATE LLM-POWERED CHATBOT WITH A TELEGRAM FOR FREE\n",
    "(source: https://www.italiens.ee/en/blog/how_to_integrate_llm_powered_chatbot_with_telegram_for_free/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4d38c3",
   "metadata": {},
   "source": [
    "# Before starting \n",
    "What You’ll Need\n",
    "- A Telegram account\n",
    "- A Hugging Face account (free)\n",
    "- Basic Python setup (Python installed, and ideally a virtual environment set up)\n",
    "\n",
    "## Step 1: Create Your Telegram Bot\n",
    "1. Open Telegram on your device.\n",
    "2. Search for BotFather and click on the ☑️ verified account.\n",
    "3. Type /newbot and follow the instructions:\n",
    "    - Give your bot a name\n",
    "    - Create a username that ends with bot\n",
    "4. BotFather will send you a bot token. Copy this token — you’ll need it soon.\n",
    "\n",
    "## Step 2: Get a Free LLM API Key from Hugging Face\n",
    "1. Go to huggingface.co and sign up (free).\n",
    "2. Click your profile icon → Access Tokens.\n",
    "3. Click New Token, give it a name (e.g., \"LLMBotToken\"), and create it with write access.\n",
    "4. Copy the API token.\n",
    "5. Choose a free model from the Hugging Face Model Hub. Free models have open license (e.g. MIT, Apache 2.0). In this guide we will use DeepSeek-R1-Distill-Qwen-1.5B.\n",
    "    - Keep in mind, this is a reasoning model, so it's quite verbose. You can \"play\" with it directly on the Hugging Face website, or choose another one.\n",
    "6. Click Deploy → Inference Providers. Here you can choose an Inference Provider to deploy selected model and access sample code snippets for Python, JavaScript, and cURL.\n",
    "\n",
    "## Step 3: Install Required Libraries\n",
    "1. Create a new folder for your project and open it in your terminal.\n",
    "2. You will need to install the following libraries to interact with the Telegram bot API and Hugging Face:\n",
    "    ```ruby\n",
    "    pip install python-telegram-bot huggingface_hub\n",
    "    ```\n",
    "\n",
    "## Step 4: Prepare the Python Telegram Bot Script\n",
    "1. Create a file named bot.py.\n",
    "2. Paste the following code inside the file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4725b5b",
   "metadata": {},
   "source": [
    "## First import libraries\n",
    "1. telegram / telegram.ext: librería oficial python-telegram-bot.\n",
    "\n",
    "    - Update: objeto que representa un evento entrante (mensaje, comando, etc.).\n",
    "    - ApplicationBuilder: constructor de la app del bot (reemplaza al viejo Updater en versiones modernas).\n",
    "    - CommandHandler: maneja comandos como /start.\n",
    "    - MessageHandler: maneja mensajes de texto normales.\n",
    "    - ContextTypes: tipo para el context que acompaña al update (útil para anotar tipos y acceder a data).\n",
    "    - filters: condiciones para MessageHandler (p. ej. filters.TEXT).\n",
    "\n",
    "2. InferenceClient de huggingface_hub: cliente para llamar a modelos alojados (aquí lo usan para chat/completions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65ddd4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "from telegram import Update\n",
    "from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, ContextTypes, filters\n",
    "from huggingface_hub import InferenceClient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c51aa5",
   "metadata": {},
   "source": [
    "## Declarar TOKENS\n",
    "- TELEGRAM_TOKEN es el Token del bot de Telegram\n",
    "- HF_API_KEY es el Token de Huggingface como inference provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11eebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TELEGRAM_TOKEN = 'YOUR_TELEGRAM_BOT_TOKEN'\n",
    "HF_API_KEY = 'YOUR_HF_API_TOKEN'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5ad894",
   "metadata": {},
   "source": [
    "## Inicialización del cliente y almacenar conversaciones\n",
    "\n",
    "La función InferenceClient(...) crea el cliente para llamar al servicio de inferencia. \"provider='novita'\" es un parametro de proveedor asociado al LLM elegido. \n",
    "\n",
    "La variable user_conversations = {} en un dict en memoria que mapea user_id con la lista de mensajes (historial para contexto). Ojo! Esto se borra al reiniciar y puede crecer indefinidamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = InferenceClient(\n",
    "    provider=\"novita\",\n",
    "    api_key=HF_API_KEY,\n",
    ")\n",
    "\n",
    "user_conversations = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c496bbea",
   "metadata": {},
   "source": [
    "## Handler del comando start \n",
    "\n",
    "Cuando un usuario manda /start, Telegram envía un Update. La función async (asíncrona) recibe ese Update y update.message.reply_text(...) envía texto de vuelta al usuario (aquí el bot da una bienvenida)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45f39cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    await update.message.reply_text('Hello! Send me a message and I will reply using AI!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4e6ada",
   "metadata": {},
   "source": [
    "## Función main y arranque del bot\n",
    "\n",
    "- ApplicationBuilder().token(...).build(): crea la aplicación del bot con tu token.\n",
    "- Registra handlers:\n",
    "    - /start → start\n",
    "    - Mensajes de texto que no sean comandos → handle_message (la expresión filters.TEXT & (~filters.COMMAND) filtra solo texto normal).\n",
    "- run_polling() inicia el \"polling\" — el bot consulta a los servidores de Telegram continuamente por nuevos updates (alternativa: webhook).\n",
    "- if __name__ == '__main__': main() permite ejecutar el script como programa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76a4fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #Crea una instancia de la aplicación del bot que se conecta a los servidores de Telegram, escucha los mensajes entrantes, \n",
    "    # llama autométicamente a las funciones que registres como \"handlers\"\n",
    "    app = ApplicationBuilder().token(TELEGRAM_TOKEN).build()\n",
    "\n",
    "    #Los handlers son reglas que dicen: \"Cuando ocurra X tipo de mensaje, ejecuta esta función\"\n",
    "    #CommandHandler es una función especual que detecta comandos de telegram tipo /help, /start, etc. \n",
    "    #Esta línea le dice al bot \"Cuando un usuario escriba el comando /start (cuyo nombre es \"start\") llama a la función start\"\n",
    "    app.add_handler(CommandHandler(\"start\", start))\n",
    "\n",
    "    #El message handler maneja mensajes de texto normales \n",
    "    #Le dice al bot \"cuando recibas un mensaje de texto que no sea un comando, llama a la función handle_message\"\n",
    "    #filters.TEXT acepta mensajes de tipo texto \n",
    "    #~filters.COMMAND excluye los comandos (mensajes que empiezan por /)\n",
    "    #El operador & combina ambos filtros\n",
    "    app.add_handler(MessageHandler(filters.TEXT & (~filters.COMMAND), handle_message))\n",
    "\n",
    "    print(\"Bot is running...\")\n",
    "    app.run_polling()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda84a7b",
   "metadata": {},
   "source": [
    "## Handler para mensajes de texto (núcleo del bot)\n",
    "1. Extrae el user_id (identificador del chat) y el ltexto del usuario. Y envía un mensaje inmediato \"Thinking\". \n",
    "2. Si no existe un historial para ese user_id, crea la lista. \n",
    "3. Agrega el mensaje del usuario al historial como {\"role\": \"user\", \"content\": ...} — este formato es el típico para APIs de chat (sistema/user/assistant).\n",
    "4. Llama a la API de Hugging Face via InferenceClient pidiendo una completación de chat. Parámetros: \n",
    "    - model: nombre del modelo \n",
    "    - messages: historial de conversación (contexto)\n",
    "    - max_tokens: límite de tokens en la respuesta\n",
    "5. Extrae el texto de respiesta del LLM (choices[0]...)\n",
    "6. Añade la respuesta al historial para mantener contexto en futuros intercambios. \n",
    "7. Envía la respuesta al usuario vía reply_text\n",
    "8. Por último captura cualquier excpeción de la llamada a la API o de la lógica y la notifica al usuario. \n",
    "\n",
    "Explicación: await update.message.reply_text(\"Thinking...\")\n",
    "- update: es un objeto de tipo de telegram.Update que representa un evento entrante que el bot recibe desde telegram. Ejemplo de lo que puede contener: \n",
    "    update = {\n",
    "        \"update_id\": 123456,\n",
    "        \"message\": {\n",
    "            \"message_id\": 42,\n",
    "            \"from\": {\"id\": 987654321, \"first_name\": \"Alice\"},\n",
    "            \"chat\": {\"id\": 987654321, \"type\": \"private\"},\n",
    "            \"date\": 1698163837,\n",
    "            \"text\": \"Hola bot!\"\n",
    "        }\n",
    "    }\n",
    "- update.message: es el mensaje concreto que envió el usuario\n",
    "- reply_text() es un método del objeto Message cuyo propósito es responder a ese mensaje (en el mismo chat) enviando texto de vuelta al usuario. Internamente: \n",
    "    - Construye una petición HTTP a la API de telegram\n",
    "    - Espera la respuesta (por eso es asíncrono)\n",
    "    - Usa el TELEGRAM_TOKEN para autentificar\n",
    "    - Envía el texto \"Thinking\"\n",
    "    - Se pausa la ejecución de la función mientras se envía el mensaje y se recibe la confirmación\n",
    "\n",
    "Qué devuelve? \n",
    "- reply_text() devuelve un objeto Message que representa el mensaje que el bot acaba de enviar.\n",
    "- Esto te permitiría: Guardar el ID del mensaje. Editarlo después (por ejemplo, cambiar “Thinking…” por la respuesta final).\n",
    "    ```ruby\n",
    "    sent_msg = await update.message.reply_text(\"Thinking...\")\n",
    "    # unos segundos después\n",
    "    await sent_msg.edit_text(\"Aquí está tu respuesta final ✨\")\n",
    "    ```\n",
    "\n",
    "EN resumen: update.message.reply_text(\"Thinking...\") → le dice al servidor de Telegram “envía este texto como respuesta al mensaje recibido”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cac91f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):\n",
    "    user_id = update.message.chat_id\n",
    "    user_message = update.message.text\n",
    "\n",
    "    # Después de mandar el mensaje, el control vuelve al bucle principal mientras Telegram responde. Cuando responde, esta función se reanuda\n",
    "    await update.message.reply_text('Thinking... 🤔')\n",
    "\n",
    "    #guardar el contexto según el user_id permite que haya varios clientes a la vez guardando el contexto/historial por separado\n",
    "    if user_id not in user_conversations:\n",
    "        user_conversations[user_id] = []\n",
    "\n",
    "    user_conversations[user_id].append({\"role\": \"user\", \"content\": user_message})\n",
    "\n",
    "    try:\n",
    "        #esta función no es asíncrona\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "            messages=user_conversations[user_id],\n",
    "            max_tokens=500,\n",
    "        )\n",
    "\n",
    "        response_text = completion.choices[0].message[\"content\"]\n",
    "        user_conversations[user_id].append({\"role\": \"assistant\", \"content\": response_text})\n",
    "\n",
    "        # Al terminar, responde al usuario\n",
    "        await update.message.reply_text(response_text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        await update.message.reply_text('Something went wrong! 😥')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAGChatBot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
